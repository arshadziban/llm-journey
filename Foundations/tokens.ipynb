{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd1dbe3f",
   "metadata": {},
   "source": [
    "# What is a Token?\n",
    "\n",
    "A **token** is the smallest unit of text that a computer can process. It can be:  \n",
    "- A **word** (e.g., `\"cat\"`),  \n",
    "- A **subword** (e.g., `\"##ing\"` from `\"jumping\"`),  \n",
    "- A **single character** (e.g., `\"A\"`),  \n",
    "- Or a **special symbol** (e.g., `[UNK]` for unknown words).  \n",
    "\n",
    "---\n",
    "\n",
    "## Examples of Tokenization\n",
    "\n",
    "| **Text**         | **Word Tokens**               | **Subword Tokens (BERT)**      | **Character Tokens**                           |\n",
    "|------------------|-------------------------------|--------------------------------|------------------------------------------------|\n",
    "| `\"I ate apples!\"` | `[\"I\", \"ate\", \"apples\", \"!\"]` | `[\"i\", \"ate\", \"apple\", \"##s\", \"!\"]` | `[\"I\", \" \", \"a\", \"t\", \"e\", \" \", \"a\", \"p\", \"p\", \"l\", \"e\", \"s\", \"!\"]` |\n",
    "| `\"ChatGPT\"`      | `[\"ChatGPT\"]` (or `[UNK]`)    | `[\"chat\", \"##g\", \"##pt\"]`       | `[\"C\", \"h\", \"a\", \"t\", \"G\", \"P\", \"T\"]`          |\n",
    "\n",
    "---\n",
    "\n",
    "## Key Idea\n",
    "\n",
    "Tokens are like **building blocks** for NLP models. Just as you break a sentence into words to understand it, a tokenizer breaks text into tokens for a computer to process.\n",
    "\n",
    "**Example:**  \n",
    "- **Input:** `\"Don't panic!\"`  \n",
    "- **Tokens:** `[\"Do\", \"n't\", \"panic\", \"!\"]` (Word-level)  \n",
    "- **Tokens:** `[\"don\", \"'\", \"t\", \"panic\", \"!\"]` (Subword-level)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860cd20c",
   "metadata": {},
   "source": [
    "# Importing the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5edfdbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Skills\\llm-journey\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e157c02",
   "metadata": {},
   "source": [
    "# BertTokenizer and Hugging Face Transformers\n",
    "\n",
    "## BertTokenizer\n",
    "\n",
    "`BertTokenizer` is a class from the **Hugging Face Transformers** library that is used to convert text into a format that can be processed by **BERT** (Bidirectional Encoder Representations from Transformers) or other transformer-based models. Essentially, it helps in preprocessing text data before passing it to the model.\n",
    "\n",
    "## Hugging Face Transformers\n",
    "\n",
    "**Hugging Face Transformers** is an open-source library developed by Hugging Face that provides easy access to a variety of state-of-the-art **natural language processing (NLP)** models, such as **BERT**, **GPT**, **T5**, and more. \n",
    "\n",
    "These models are pre-trained on massive datasets and can be used for a wide range of tasks, including:\n",
    "- Text classification\n",
    "- Sentiment analysis\n",
    "- Question answering\n",
    "- Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293708dc",
   "metadata": {},
   "source": [
    "# Loading the Pretrained Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "285d3736",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Skills\\llm-journey\\env\\lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Arshad Ziban\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117cf2c2",
   "metadata": {},
   "source": [
    "# `.from_pretrained(\"bert-base-uncased\")`\n",
    "\n",
    "- This means you're downloading a **pre-trained tokenizer** that matches the `\"bert-base-uncased\"` model.\n",
    "- `\"bert-base-uncased\"` is one version of BERT. \n",
    "  - The word **\"uncased\"** means that this tokenizer will treat all words as lowercase (e.g., \"Apple\" and \"apple\" will be treated the same).\n",
    "\n",
    "## What does it do?\n",
    "\n",
    "- When you use this line of code, the `tokenizer` object will know how to:\n",
    "  1. Take any sentence you give it.\n",
    "  2. Split it into smaller parts (**tokens**).\n",
    "  3. Convert those tokens into numbers (**IDs**). \n",
    "\n",
    "These numbers are what the BERT model needs to work properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0d4311",
   "metadata": {},
   "source": [
    "# Defining Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855c3647",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
